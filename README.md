# Sparkå®éªŒæ¼”ç¤ºè„šæœ¬ï¼ˆå«è®²è§£+æŒ‡ä»¤è¡Œï¼‰
## ä¸€ã€å®éªŒå¹³å°ç‰ˆæœ¬
- æ“ä½œç³»ç»Ÿï¼šUbuntu 24.04.2 LTS
- Hadoopç‰ˆæœ¬ï¼š3.3.5
- JDKç‰ˆæœ¬ï¼š1.8.0_371
- Sparkç‰ˆæœ¬ï¼š3.4.0

æ¥ä¸‹æ¥æˆ‘ä¼šæŒ‰æ­¥éª¤æ¼”ç¤ºæ¯ä¸ªç¯èŠ‚çš„æ“ä½œåŠç»“æœéªŒè¯

## äºŒã€ç¬¬ä¸€éƒ¨åˆ†ï¼šSparkå®‰è£…åŠç¯å¢ƒé…ç½®
### è®²è§£è¦ç‚¹
1. é‡‡ç”¨aria2cå¤šçº¿ç¨‹ä¸‹è½½Sparkå®‰è£…åŒ…ï¼Œæå‡ä¸‹è½½é€Ÿåº¦ï¼›
2. è§£å‹å®‰è£…åé…ç½®ç¯å¢ƒå˜é‡ï¼Œå…³è”Hadoopç±»è·¯å¾„ï¼›
3. é€šè¿‡è¿è¡Œå®˜æ–¹ç¤ºä¾‹ç¨‹åºSparkPiéªŒè¯å®‰è£…æˆåŠŸï¼ˆè¾“å‡ºÏ€è¿‘ä¼¼å€¼ï¼‰ã€‚

### æŒ‡ä»¤è¡Œæ“ä½œï¼ˆæŒ‰é¡ºåºæ‰§è¡Œï¼‰
```bash
# 1. å¤šçº¿ç¨‹ä¸‹è½½Sparkå®‰è£…åŒ…ï¼ˆ-x 16è®¾ç½®æœ€å¤§è¿æ¥æ•°ï¼Œ-s 16è®¾ç½®æœ€å¤§åˆ†ç‰‡æ•°ï¼‰
aria2c -x 16 -s 16 https://archive.apache.org/dist/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz

# 2. è§£å‹åˆ°/usr/localç›®å½•ï¼ˆå‡è®¾ä¸‹è½½åˆ°æ¡Œé¢ï¼‰
sudo tar -zxvf ~/æ¡Œé¢/spark-3.4.0-bin-hadoop3.tgz -C /usr/local/

# 3. é‡å‘½åä¸ºsparkï¼ˆç®€åŒ–è·¯å¾„ï¼‰
sudo mv /usr/local/spark-3.4.0-bin-hadoop3 /usr/local/spark

# 4. é…ç½®Sparkç¯å¢ƒå˜é‡ï¼ˆå…³è”Hadoopç±»è·¯å¾„ï¼‰
echo "export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)" >> /usr/local/spark/conf/spark-env.sh

# 5. éªŒè¯å®‰è£…ï¼ˆè¿è¡ŒSparkPiç¤ºä¾‹ï¼‰
cd /usr/local/spark
bin/run-example SparkPi
```

### é¢„æœŸç»“æœ
è¾“å‡ºÏ€çš„è¿‘ä¼¼å€¼ï¼ˆå¦‚ï¼š3.1394956974784876ï¼‰ï¼ŒåŒæ—¶æ—¥å¿—æ˜¾ç¤ºSparkContextæ­£å¸¸å¯åŠ¨å¹¶åœæ­¢ï¼Œæ— æŠ¥é”™ã€‚

## ä¸‰ã€ç¬¬äºŒéƒ¨åˆ†ï¼šç†Ÿæ‚‰å¸¸ç”¨SparkShellå‘½ä»¤
### è®²è§£è¦ç‚¹
1. SparkShellæ˜¯äº¤äº’å¼å‘½ä»¤è¡Œå·¥å…·ï¼Œæ”¯æŒScalaè¯­æ³•ï¼Œè‡ªåŠ¨åˆ›å»ºSparkContextï¼ˆscï¼‰å’ŒSparkSessionï¼ˆsparkï¼‰ï¼›
2. æ¼”ç¤ºRDDçš„æ ¸å¿ƒæ“ä½œï¼šåŠ è½½æ–‡ä»¶ã€è®¡æ•°ã€å–é¦–è¡Œã€è¿‡æ»¤ã€è½¬æ¢è®¡ç®—ç­‰ï¼›
3. æ‰€æœ‰æ“ä½œåŸºäºå¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ï¼ˆRDDï¼‰ï¼Œä½“ç°Sparkçš„æƒ°æ€§è®¡ç®—ç‰¹æ€§ã€‚

### æŒ‡ä»¤è¡Œæ“ä½œï¼ˆæŒ‰é¡ºåºæ‰§è¡Œï¼‰
```bash
# 1. å¯åŠ¨SparkShellï¼ˆlocal[*]è¡¨ç¤ºä½¿ç”¨æœ¬åœ°æ‰€æœ‰CPUæ ¸å¿ƒï¼‰
spark-shell
# å¯åŠ¨SparkShellï¼Œè®¾ç½®æ—¥å¿—çº§åˆ«ä¸ºWARNï¼ˆå‡å°‘å†—ä½™æ—¥å¿—ï¼Œåªçœ‹å…³é”®ä¿¡æ¯ï¼‰ï¼Œä½¿ç”¨æœ¬åœ°æ‰€æœ‰CPUæ ¸å¿ƒ
spark-shell --master local[*] --conf spark.driver.extraJavaOptions="-Dlog4j.configuration=file:///usr/local/spark/conf/log4j2.properties"
```

- å¯åŠ¨æˆåŠŸåï¼Œæ§åˆ¶å°ä¼šæ˜¾ç¤ºï¼š`Spark context available as 'sc'` å’Œ `Spark session available as 'spark'`ï¼Œè¯´æ˜ç¯å¢ƒåˆå§‹åŒ–å®Œæˆã€‚

è¿›å…¥Scalaäº¤äº’ç¯å¢ƒåï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
```scala
# 1. åŠ è½½Sparkè‡ªå¸¦çš„README.mdæ–‡ä»¶ï¼ˆæœ¬åœ°è·¯å¾„ï¼Œå‰ç¼€file://ä¸å¯çœç•¥ï¼‰ï¼Œåˆ›å»ºæ–‡æœ¬RDD
val textFile = sc.textFile("file:///usr/local/spark/README.md")
# è¾“å‡ºæç¤ºï¼štextFile: org.apache.spark.rdd.RDD[String] = file:///usr/local/spark/README.md MapPartitionsRDD[1] at textFile at <console>:23

# 2. æŸ¥çœ‹RDDçš„åˆ†åŒºæ•°ï¼ˆé»˜è®¤ä¸CPUæ ¸å¿ƒæ•°ä¸€è‡´ï¼Œæœ¬åœ°æ¨¡å¼ä¸‹ä¸ºlocal[*]çš„æ ¸å¿ƒæ•°ï¼‰
textFile.getNumPartitions
# é¢„æœŸè¾“å‡ºï¼šInt = 2

# 3. ç»Ÿè®¡RDDçš„è¡Œæ•°ï¼ˆcountæ“ä½œè§¦å‘è®¡ç®—ï¼‰
textFile.count()  # é¢„æœŸè¾“å‡ºï¼š125

# 4. è·å–RDDçš„ç¬¬ä¸€è¡Œå†…å®¹
textFile.first()  # é¢„æœŸè¾“å‡ºï¼š# Apache Spark

# 5. è¿‡æ»¤å‡ºåŒ…å«"Spark"çš„è¡Œï¼Œè¿”å›æ–°RDDå¹¶ç»Ÿè®¡è¡Œæ•°
val sparkLines = textFile.filter(line => line.contains("Spark"))
sparkLines.count()  # é¢„æœŸè¾“å‡ºï¼šåŒ…å«"Spark"çš„è¡Œæ•°ï¼ˆçº¦30+ï¼‰

# 6. è®¡ç®—æ¯è¡Œçš„å•è¯æ•°ï¼Œæ‰¾å‡ºæœ€å¤§å€¼
textFile.map(line => line.split(" ").length).reduce((a, b) => math.max(a, b))  # é¢„æœŸè¾“å‡ºï¼šæ¯è¡Œæœ€å¤§å•è¯æ•°ï¼ˆçº¦20+ï¼‰

# 7. é€€å‡ºSparkShell
:quit
```

### è¡ŒåŠ¨æ“ä½œï¼šè§¦å‘è®¡ç®—ï¼Œè·å–ç»“æœï¼ˆä½“ç°æƒ°æ€§è®¡ç®—ï¼‰
è¡ŒåŠ¨æ“ä½œä¼šè§¦å‘ä¹‹å‰æ‰€æœ‰è½¬æ¢æ“ä½œçš„æ‰§è¡Œï¼Œè¿”å›å…·ä½“ç»“æœæˆ–å†™å…¥å¤–éƒ¨å­˜å‚¨ã€‚
```scala
# 1 ç»Ÿè®¡RDDæ€»è¡Œæ•°ï¼ˆåŸºç¡€è¡ŒåŠ¨æ“ä½œï¼Œè§¦å‘è®¡ç®—ï¼‰
val totalLines = textFile.count()
# é¢„æœŸè¾“å‡ºï¼štotalLines: Long = 125ï¼ˆä¸å®éªŒæ–‡æ¡£ä¸€è‡´ï¼Œç¡®è®¤æ–‡ä»¶åŠ è½½æ­£ç¡®ï¼‰

# 2 è·å–RDDçš„å‰5è¡Œå†…å®¹ï¼ˆæŠ½æ ·æŸ¥çœ‹ï¼Œé¿å…æ‰“å°æ‰€æœ‰æ•°æ®ï¼‰
textFile.take(5)
# é¢„æœŸè¾“å‡ºï¼š
# Array(
#   "# Apache Spark", 
#   "", 
#   "Spark is a unified analytics engine for large-scale data processing. It provides", 
#   "high-level APIs in Scala, Java, Python, and R, and an optimized engine that", 
#   "supports general computation graphs for data analysis."
# )

# 3 æ£€æŸ¥æ˜¯å¦åŒ…å«æŒ‡å®šå†…å®¹ï¼ˆåˆ¤æ–­RDDä¸­æ˜¯å¦æœ‰å«"Python"çš„è¡Œï¼‰
val hasPython = textFile.filter(line => line.contains("Python")).isEmpty
# é¢„æœŸè¾“å‡ºï¼šhasPython: Boolean = falseï¼ˆè¯´æ˜æœ‰å«"Python"çš„è¡Œï¼‰
textFile.filter(line => line.contains("Python")).take(1)  # æŸ¥çœ‹å…¶ä¸­ä¸€è¡Œ
# é¢„æœŸè¾“å‡ºï¼šArray("high-level APIs in Scala, Java, Python, and R, and an optimized engine that")
```


### è½¬æ¢æ“ä½œï¼šæ„å»ºè®¡ç®—é€»è¾‘ï¼ˆä¸è§¦å‘æ‰§è¡Œï¼‰
è½¬æ¢æ“ä½œä¼šç”Ÿæˆæ–°çš„RDDï¼Œè®°å½•ä¸çˆ¶RDDçš„ä¾èµ–å…³ç³»ï¼Œä¸ç«‹å³æ‰§è¡Œè®¡ç®—ï¼Œä»…åœ¨è¡ŒåŠ¨æ“ä½œè§¦å‘æ—¶æ‰è¿è¡Œã€‚
```scala
# 1 è¿‡æ»¤éç©ºè¡Œï¼ˆå»é™¤READMEä¸­çš„ç©ºè¡Œï¼Œç”Ÿæˆæ–°RDDï¼‰
val nonEmptyLines = textFile.filter(line => !line.trim.isEmpty)
# è¾“å‡ºæç¤ºï¼šnonEmptyLines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at filter at <console>:25
# æ­¤æ—¶æœªè§¦å‘è®¡ç®—ï¼Œéœ€åç»­è¡ŒåŠ¨æ“ä½œè§¦å‘

# 2 æ¯è¡Œå•è¯æ‹†åˆ†ï¼ˆflatMapä¸mapçš„åŒºåˆ«ï¼šflatMapä¼šå°†æ•°ç»„â€œæ‰å¹³åŒ–â€ï¼Œé€‚åˆå•è¯æ‹†åˆ†ï¼‰
// mapï¼šæ¯è¡Œè¿”å›ä¸€ä¸ªå•è¯æ•°ç»„ï¼ŒRDDç±»å‹ä¸ºRDD[Array[String]]
val wordArrays = nonEmptyLines.map(line => line.split(" "))
// flatMapï¼šæ¯è¡Œçš„å•è¯æ•°ç»„æ‹†åˆ†ä¸ºå•ä¸ªå•è¯ï¼ŒRDDç±»å‹ä¸ºRDD[String]ï¼ˆæœ€ç»ˆæ‰€æœ‰å•è¯åœ¨ä¸€ä¸ªRDDä¸­ï¼‰
val words = nonEmptyLines.flatMap(line => line.split(" "))
# è¾“å‡ºæç¤ºï¼šwords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at flatMap at <console>:25

# 3 å•è¯å»é‡ï¼ˆç»Ÿè®¡ä¸é‡å¤çš„å•è¯æ•°é‡ï¼‰
val distinctWords = words.distinct()
# è¾“å‡ºæç¤ºï¼šdistinctWords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at distinct at <console>:25
# æ‰“å°å»é‡åå•è¯
distinctWords.foreach(println)
```


### ç»¼åˆæ“ä½œï¼šè½¬æ¢+è¡ŒåŠ¨ç»“åˆï¼ˆå®æˆ˜åœºæ™¯ï¼‰
#### åœºæ™¯1ï¼šå•è¯é¢‘æ¬¡ç»Ÿè®¡ï¼ˆç»å…¸æ¡ˆä¾‹ï¼‰
```scala
# 1 ç»Ÿè®¡æ¯ä¸ªå•è¯å‡ºç°çš„æ¬¡æ•°ï¼ˆæ­¥éª¤ï¼šå•è¯â†’(å•è¯,1)â†’æŒ‰å•è¯èšåˆæ±‚å’Œï¼‰
  // ç»Ÿä¸€è½¬ä¸ºå°å†™ï¼Œé¿å…"Spark"å’Œ"spark"è¢«è§†ä¸ºä¸åŒå•è¯
  // æŒ‰å•è¯ï¼ˆkeyï¼‰èšåˆï¼Œvalueç´¯åŠ ï¼ˆç»Ÿè®¡æ¬¡æ•°ï¼‰
val wordCounts = words
  .map(word => (word.toLowerCase, 1))
  .reduceByKey((a, b) => a + b)

# 2 æŸ¥çœ‹å‡ºç°æ¬¡æ•°å‰10çš„å•è¯ï¼ˆæŒ‰æ¬¡æ•°é™åºæ’åºï¼‰
val top10Words = wordCounts.sortBy(_._2, ascending = false).take(10)
top10Words.foreach(println)  # å¾ªç¯æ‰“å°ç»“æœ
# é¢„æœŸè¾“å‡ºï¼ˆç¤ºä¾‹ï¼Œå…·ä½“æ¬¡æ•°å¯èƒ½ç•¥æœ‰å·®å¼‚ï¼‰ï¼š
# (the,22)
# (a,18)
# (spark,15)
# (for,12)
# (and,10)
# (to,9)
# (in,8)
# (is,7)
# (data,6)
# (processing,5)
```

## å››ã€ç¬¬ä¸‰éƒ¨åˆ†ï¼šåŸºäºSpark APIçš„ç‹¬ç«‹åº”ç”¨å¼€å‘
### è®²è§£è¦ç‚¹
1. åˆ†åˆ«ä½¿ç”¨Javaå’ŒScalaè¯­è¨€å¼€å‘Sparkç‹¬ç«‹åº”ç”¨ï¼Œæ ¸å¿ƒé€»è¾‘ä¸ºç»Ÿè®¡æ–‡ä»¶ä¸­å«"a"å’Œ"b"çš„è¡Œæ•°ï¼›
2. ä½¿ç”¨Mavenè¿›è¡Œé¡¹ç›®æ„å»ºå’Œæ‰“åŒ…ï¼Œè§£å†³Sparkä¾èµ–é—®é¢˜ï¼›
3. é€šè¿‡spark-submitè„šæœ¬æäº¤åº”ç”¨ï¼ŒéªŒè¯è¿è¡Œç»“æœã€‚

### æ¨¡å—1ï¼šJavaç‹¬ç«‹åº”ç”¨ï¼ˆSparkApp2ï¼‰
#### æ­¥éª¤1ï¼šåˆ›å»ºé¡¹ç›®ç»“æ„åŠä»£ç 
```bash
# 1. åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„
mkdir -p ./sparkapp2/src/main/java

# 2. ç¼–å†™Javaåº”ç”¨ä»£ç ï¼ˆSimpleApp.javaï¼‰
vim ./sparkapp2/src/main/java/SimpleApp.java
```
å°†ä»¥ä¸‹ä»£ç å¤åˆ¶åˆ°æ–‡ä»¶ä¸­ï¼š
```java
/*** SimpleApp.java ***/
import org.apache.spark.api.java.*;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.SparkConf;
 
public class SimpleApp {
    public static void main(String[] args) {
        String logFile = "file:///usr/local/spark/README.md"; // Should be some file on your system
        SparkConf conf=new SparkConf().setMaster("local").setAppName("SimpleApp");
        JavaSparkContext sc=new JavaSparkContext(conf);
        JavaRDD<String> logData = sc.textFile(logFile).cache(); 
        long numAs = logData.filter(new Function<String, Boolean>() {
            public Boolean call(String s) { return s.contains("a"); }
        }).count(); 
        long numBs = logData.filter(new Function<String, Boolean>() {
            public Boolean call(String s) { return s.contains("b"); }
        }).count(); 
        System.out.println("Lines with a: " + numAs + ", lines with b: " + numBs);
    }
}
```

#### æ­¥éª¤2ï¼šç¼–å†™Mavenä¾èµ–æ–‡ä»¶ï¼ˆpom.xmlï¼‰
```bash
# è¿›å…¥é¡¹ç›®æ ¹ç›®å½•
cd ~/sparkapp2
# åˆ›å»ºpom.xmlæ–‡ä»¶
vim pom.xml
```
å¤åˆ¶ä»¥ä¸‹ä¾èµ–å†…å®¹ï¼š
```xml
<project>
    <groupId>cn.edu.xmu</groupId>
    <artifactId>simple-project</artifactId>
    <modelVersion>4.0.0</modelVersion>
    <name>Simple Project</name>
    <packaging>jar</packaging>
    <version>1.0</version>
    <repositories>
        <repository>
            <id>jboss</id>
            <name>JBoss Repository</name>
            <url>http://repository.jboss.com/maven2/</url>
        </repository>
    </repositories>
    <dependencies>
        <dependency> <!-- Spark dependency -->
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.12</artifactId>
            <version>3.4.0</version>
        </dependency>
    </dependencies>
</project>
```

#### æ­¥éª¤3ï¼šæ‰“åŒ…å¹¶æ‰§è¡Œåº”ç”¨
```bash
# 1. æ‰“åŒ…ï¼ˆéœ€æå‰å®‰è£…Mavenï¼ŒéªŒè¯ï¼šmvn -vï¼‰
cd ~/sparkapp2
find .
cd ~/sparkapp2
/usr/local/maven/bin/mvn package

# 2. ä½¿ç”¨spark-submitæäº¤åº”ç”¨
/usr/local/spark/bin/spark-submit --class "SimpleApp" ~/sparkapp2/target/simple-project-1.0.jar 2>&1 | grep "Lines with a"
```

#### é¢„æœŸç»“æœ
è¾“å‡ºï¼š`Lines with a: 72, lines with b: 39`

### æ¨¡å—2ï¼šScalaç‹¬ç«‹åº”ç”¨ï¼ˆSparkApp3ï¼‰
#### æ­¥éª¤1ï¼šåˆ›å»ºé¡¹ç›®ç»“æ„åŠä»£ç 
```bash
# 1. åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„
mkdir -p ./sparkapp3/src/main/scala

# 2. ç¼–å†™Scalaåº”ç”¨ä»£ç ï¼ˆSimpleApp.scalaï¼‰
vim ./sparkapp3/src/main/scala/SimpleApp.scala
```
å°†ä»¥ä¸‹ä»£ç å¤åˆ¶åˆ°æ–‡ä»¶ä¸­ï¼š
```scala
/* SimpleApp.scala */
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
 
object SimpleApp {
        def main(args: Array[String]) {
            val logFile = "file:///usr/local/spark/README.md" // Should be some file on your system
            val conf = new SparkConf().setAppName("Simple Application")
            val sc = new SparkContext(conf)
            val logData = sc.textFile(logFile, 2).cache()
            val numAs = logData.filter(line => line.contains("a")).count()
            val numBs = logData.filter(line => line.contains("b")).count()
            println("Lines with a: %s, Lines with b: %s".format(numAs, numBs))
        }
    }
```

#### æ­¥éª¤2ï¼šç¼–å†™Mavenä¾èµ–æ–‡ä»¶ï¼ˆpom.xmlï¼‰
```bash
# è¿›å…¥é¡¹ç›®æ ¹ç›®å½•
cd ~/sparkapp3
# åˆ›å»ºpom.xmlæ–‡ä»¶
vim pom.xml
```
å¤åˆ¶ä»¥ä¸‹ä¾èµ–å†…å®¹ï¼š
```xml
<project>
    <groupId>cn.edu.xmu</groupId>
    <artifactId>simple-project</artifactId>
    <modelVersion>4.0.0</modelVersion>
    <name>Simple Project</name>
    <packaging>jar</packaging>
    <version>1.0</version>
    <repositories>
        <repository>
            <id>jboss</id>
            <name>JBoss Repository</name>
            <url>http://repository.jboss.com/maven2/</url>
        </repository>
    </repositories>
    <dependencies>
        <dependency> <!-- Spark dependency -->
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.12</artifactId>
            <version>3.4.0</version>
        </dependency>
    </dependencies>

  <build>
    <sourceDirectory>src/main/scala</sourceDirectory>
    <plugins>
      <plugin>
        <groupId>org.scala-tools</groupId>
        <artifactId>maven-scala-plugin</artifactId>
        <executions>
          <execution>
            <goals>
              <goal>compile</goal>
            </goals>
          </execution>
        </executions>
        <configuration>
          <scalaVersion>2.12.17</scalaVersion>
          <args>
            <arg>-target:jvm-1.8</arg>
          </args>
        </configuration>
    </plugin>
    </plugins>
</build>
</project>
```

#### æ­¥éª¤3ï¼šæ‰“åŒ…å¹¶æ‰§è¡Œåº”ç”¨
```bash
# 1. æ‰“åŒ…
cd  ~/sparkapp3
/usr/local/maven/bin/mvn  package
# 2. æäº¤åº”ç”¨åˆ°Spark
/usr/local/spark/bin/spark-submit --class "SimpleApp" ~/sparkapp3/target/simple-project-1.0.jar 2>&1 | grep "Lines with a:"
```

#### é¢„æœŸç»“æœ
è¾“å‡ºï¼š`Lines with a: 72, Lines with b: 39`ï¼ˆä¸Javaåº”ç”¨ç»“æœä¸€è‡´ï¼ŒéªŒè¯ä»£ç æ­£ç¡®æ€§ï¼‰


### å¦‚ä½•åˆ‡æ¢è¿è¡Œ Java æˆ– Scala ç¨‹åº

#### ğŸ”¹ è¿è¡Œ Java ç‰ˆæœ¬
1. ä¿®æ”¹ pom è®¾ç½® Java MainClassï¼š
```xml
<mainClass>com.example.SparkJavaApp</mainClass>
```

2. æ‰“åŒ…
```bash
mvn clean package
```

3. æ‰§è¡Œ
```bash
spark-submit \
 --class com.example.SparkJavaApp \
 target/spark-maven-app-1.0-SNAPSHOT-jar-with-dependencies.jar \
 file:///usr/local/spark/README.md
```

#### ğŸ”¹ è¿è¡Œ Scala ç‰ˆæœ¬
1. åªéœ€ä¿®æ”¹ pom ä¸­ `<mainClass>` ä¸º Scala ç±»ï¼š
```xml
<mainClass>com.example.SparkScalaApp</mainClass>
```

2. é‡æ–°æ‰“åŒ…ï¼š
```bash
mvn clean package
```

3. è¿è¡Œæ–¹å¼ï¼š
```bash
spark-submit \
 --class com.example.SparkScalaApp \
 target/spark-maven-app-1.0-SNAPSHOT-jar-with-dependencies.jar \
 file:///usr/local/spark/README.md
```

